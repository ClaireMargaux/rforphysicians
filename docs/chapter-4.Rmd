---
title: "Chapter 4"
date: "`r Sys.Date()`"
contact: claire.descombes@insel.ch
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    theme: paper
    code_folding: show
    math_method: katex
subtitle: "R for physicians"
bibliography: /home/claire/Documents/GitHub/rforphysicians/docs/Rforphysicians.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {rmarkdown::render(input)})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE,
  message = FALSE, 
  fig.height = 5,
  fig.width = 6,
  eval = TRUE
)
library(tidyverse)
```

| Author           |                                                                 |
|:-----------------|:----------------------------------------------------------------|
| Name             | Claire Descombes                                                |
| Affiliation      | UniversitÃ¤tsklinik fÃ¼r Neurochirurgie, Inselspital Bern         |
| Degree           | MSc Statistics and Data Science, University of Bern             |
| Contact          | [claire.descombes@insel.ch](mailto:claire.descombes@insel.ch)   |

The reference material for this course, as well as some useful literature to deepen your knowledge of R, can be found at the bottom of the page.

---

To explore Râ€™s statistical tools, weâ€™ll continue working with the NHANES datasets. Specifically, weâ€™ll use the merged data frame that combines the `demo`, `bpx`, `bmx` and `smq` data sets. If you still have it loaded from Chapters 2 und 3, you can use it directly. Otherwise, you can download it from the [`data_sets` folder](https://github.com/ClaireMargaux/rforphysicians/tree/main/data_sets) and import it into R.

To be able to use survival data, we will also need the NHANES data set complemented with mortality data. If you still have it loaded from Chapters 3, you can use it directly. Otherwise, you can download it from the [`data_sets` folder](https://github.com/ClaireMargaux/rforphysicians/tree/main/data_sets) and import it into R.

```{r, echo=TRUE}
# Load the merged_nhanes CSV file
merged_nhanes <- read.csv("/home/claire/Documents/GitHub/rforphysicians/data_sets/merged_nhanes.csv")

# Load the merged_nhanes_with_mort CSV file
merged_nhanes_with_mort <- read.csv("/home/claire/Documents/GitHub/rforphysicians/data_sets/merged_nhanes_with_mort.csv")
```

# Most commonly used statistical tests and models

This chapter will follow the structure outlined below. We will learn how to perform a selection of statistical tests and models in R, categorising them by their objective (e.g. comparing two means or analysing survival).

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Section</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Topic</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Subtopics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.1 Tests for comparing two groups</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests comparing means or proportions between two groups</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Studentâ€™s t-test <br> â€¢ Wilcoxon-Mann-Whitney test (Mann-Whitney-U-test) <br> â€¢ Chi-Squared test  <br> â€¢ Fisherâ€™s exact test <br> â€¢ McNemar test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.2 Tests for more than two groups</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for comparing multiple groups</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Kruskal-Wallis test <br> â€¢ Friedman test <br> â€¢ Chi-Squared test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.3 Tests for distribution and normality</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for distribution of data</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Lilliefors / Kolmogorov-Smirnov-Lilliefors test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.4 Tests for survival analysis</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for time-to-event data</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Logrank test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.5 Correlation and association tests</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for relationships between variables</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Correlation test by Pearson <br> â€¢ Correlation test by Spearman</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.6 Predictive modelling and regression</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Predictive models and regression techniques</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Generalized Linear Models (GLMs): Linear regression, logistic regression, ordinal regression, Cox (proportional hazards) regression - Multivariable Regression <br> â€¢ Mixed Effects Models <br> â€¢ Generalized Additive Models (GAMs) <br> â€¢ Generalized Additive Mixed Models (GAMMs)</td>
    </tr>
  </tbody>
</table>

## Which test for which data?

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics(path = "images/types_of_data.jpg")
```

Before we begin, here is an overview of the type of data required by each test/model. More tests and models are presented than will be discussed in the course, but I have included them for the sake of completeness. If you feel lost on that topic at any point in the chapter, just scroll up again to have a look at this table.

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;"></th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">1 sample</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">2 paired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">2 unpaired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">&gt;2 paired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">&gt;2 unpaired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Continuous predictor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Binary</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Binomial test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ McNemar test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square test<br>â€¢ Fisher's exact test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Cochran's Q test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square test<br>â€¢ Extensions of Fisher's test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Logistic regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Nominal</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square goodness of fit test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square test<br>â€¢ Extensions of Fisher's test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Multinomial regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Ordinal</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Wilcoxon signed-rank test<br>â€¢ Sign test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Sign test<br>â€¢ Wilcoxon signed-rank test on differences</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Mann-Whitney U test (Wilcoxon rank-sum test)</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Friedman test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Kruskalâ€“Wallis test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Ordinal regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Continuous</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ One-sample t-test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Paired t-test<br>â€¢ Wilcoxon signed-rank test on differences</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Two-sample t-test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Repeated measures ANOVA</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ ANOVA</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Linear regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Time-to-event</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ One-sample log-rank test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Log-rank test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Cox regression<br>â€¢ Weibull regression</td>
    </tr>
  </tbody>
</table>

## What is a statistical test?

Before we start looking at many different <u>statistical hypothesis tests</u>, let us recall the basic structure that they all share, by exploring more in-depth the Student's t-test for two, independent continuous samples.

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Generally</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Student's t-test for two, independent samples</th>
      </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">A test always aims to put to test a claim we make about some parameter (a correlation, a difference in mean, etc.) in the distribution of the population of interest.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Working on two patients cohorts, the smokers and the non-smokers, one could ask her-/himself if they share the same mean on a continuous variable, e.g. the BMI</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The data itself is expected to fulfil some **assumptions**.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">As the BMI data is continuous, the cohorts are big (> 100 patients each, of sizes $n_1$ and $n_2$) and assumed to be independent, and there is no reason to assume they'd have a different variance in the BMI, an unpaired Student's t-test appears to be the right choice.</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The 'status quo'/no difference formulation of the question we're asking is called the **null hypothesis**.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The null hypothesis, in our example, is the hypothesis that our two cohorts (the smokers and the non-smokers) share the same mean BMI</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The hypothesis we are usually interested to prove, the one showing a difference between two quantities, is called the **alternative hypothesis**</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The alternative hypothesis, in our example, is the hypothesis that our two cohorts (the smokers and the non-smokers) do not share the same mean BMI</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Assuming the null hypothesis is true, a specific parameter, called the **statistic**, is expected to fall into a given **distribution**</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the null hypothesis is true, then the t statistic: $t = \frac{\tilde{X}_1-\tilde{X}_2}{s_p \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$ (where $s_p$ is the pooled standard deviation) follows a Student's t-distribution with $n_1+n_2-2$ degrees of freedom.</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">For any object of which the distribution is (assumed to be) known, it is possible to compute its probability to fall within a certain range; for a given **significance level** (denoted by $\alpha$) and between 0 and 1, one can determine an interval in which the statistic has a probability of $1-\alpha$ to fall within</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Letting $\alpha$ be 0.05, as it is common in medicine, we can determine that our t statistic has a probability of 95% to be in a given interval (that can be computed in R or using a table).</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the statistic, after being computed, falls in the $1-\alpha$ confidence interval ($p>\alpha$), then we **cannot reject the null hypothesis**, and have to look for further evidence to prove our claim.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the t statistic falls into the range we computed, then we cannot say that we saw a significant difference in the means.</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the statistic, after being computed, does *not* fall in the $1-\alpha$ confidence interval ($p \leq \alpha$), then we can reject the null hypothesis, and say that the claim we made on the data is **statistically significant** with a $1-\alpha$ confidence.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the t-statistic does not fall into the range we computed, then we can say that we observe a statistically significant difference in the means, with a confidence of 95%.</td>
    </tr>    
  </tbody>
</table>

A few important comments: 

  + The p-value is the **probability under the null hypothesis** of obtaining a (real-valued) test statistic **at least as extreme** as the one obtained.
  + Loosely speaking, rejection of the null hypothesis implies that there is **sufficient evidence against it**. 
  + It is not possible to prove that the null hypothesis is true: it could always be that with a bigger effect size and/or a larger sample size, we could statistically show the difference that we are trying to demonstrate. So, when a test comes out "negatively" (no statistical significance, $p>\alpha$), it does not mean that our claim is necessarly wrong.
  + On the other hand, even a significant test ($p\leq\alpha$) does not mean that we *irrefutably proved* our claim: in a theoretical world, where we could repeat our experiment 100 times in the same conditions and on the same population, even if our theory is wrong, we would always find a "positive" result ($p\leq\alpha$) in $\alpha\%$ of the cases. That is why consensus, in science, always relies on repeated experiments that (for the vast majority of them*) draw the same conclusion.
  + \* Why is that? Because again, even in a theoretical world, where we could repeat our experiment 100 times in the same conditions, even if our theory is right, we would always find a "negative" result in $\alpha\%$ of the cases. 

```{r, echo=FALSE}
# parameters
mu <- 0
sigma <- 1

# significance: tails that are 2.5% each (two-tailed alpha = 0.05)
alpha <- 0.05
lower_cut <- qnorm(alpha/2, mean = mu, sd = sigma)
upper_cut <- qnorm(1 - alpha/2, mean = mu, sd = sigma)

# density data
x <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)
df <- data.frame(x = x, y = dnorm(x, mean = mu, sd = sigma))

# split regions
df_mid   <- df[df$x >  lower_cut & df$x <  upper_cut, ]
df_left  <- df[df$x <= lower_cut, ]
df_right <- df[df$x >= upper_cut, ]

# plot: middle first, tails after (so they sit on top), line on top of everything
ggplot(df, aes(x = x, y = y)) +
  geom_area(data = df_mid,   fill = "lightgreen", alpha = 0.6) +   # middle area
  geom_area(data = df_left,  fill = "coral",  alpha = 0.8) +   # left tail (on top)
  geom_area(data = df_right, fill = "coral",  alpha = 0.8) +   # right tail (on top)
  geom_line(size = 1) +
  geom_vline(xintercept = c(lower_cut, upper_cut), linetype = "dashed") +
  annotate("text", x = lower_cut, y = max(df$y)*0.9, label = sprintf("%.3f", lower_cut), hjust = 1.1) +
  annotate("text", x = upper_cut, y = max(df$y)*0.9, label = sprintf("%.3f", upper_cut), hjust = -0.1) +
  annotate("text", 
           x = (lower_cut + upper_cut)/2, 
           y = max(df$y)*0.5, 
           label = "95%", 
           size = 6, 
           fontface = "bold", 
           color = "black") +
  annotate("text", 
           x = lower_cut-1, 
           y = max(df$y)*0.5, 
           label = "2.5%", 
           size = 6, 
           fontface = "bold", 
           color = "black") +
  annotate("text", 
           x = upper_cut+1, 
           y = max(df$y)*0.5, 
           label = "2.5%", 
           size = 6, 
           fontface = "bold", 
           color = "black") +
  labs(title = "Standard normal density",
       x = "x", y = "Density") +
  theme_minimal(base_size = 14)
```

# Tests for comparing two groups

## Continuous variables

### Student's t-test

<u>1. Null hypothesis</u>

* Means of two populations are equal

<u>2. Type of data</u>

* One or two vectors of continuous variables (e.g. blood pressure in mmHg in two cohorts)

<u>3. Requirements</u>

* Both samples are *approximately normally distributed*. Under weak assumptions, this follows in large samples from the central limit theorem, even when the distribution of observations in each group is non-normal.
* If using Student's original definition of the t-test, the two populations being compared should have the *same variance* (assessable graphically using a Qâ€“Q plot). If the sample sizes in the two groups being compared are equal, Student's original t-test is highly robust to the presence of unequal variances. *Welch's t-test* is insensitive to equality of the variances regardless of whether the sample sizes are similar.
* The data used to carry out the test should either be *sampled independently* from the two populations being compared or be fully paired. This is in general not testable from the data, but if the data are known to be dependent (e.g. paired by test design), a paired test has to be applied.

ðŸ’¡ Independent samples are randomly selected so that their observations are not dependent on the values of other observations.

<u>4. R function</u>

`t.test(x, ...)`: Performs one and two sample t-tests on vectors of data.

<u>5. Important arguments</u>

```
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)
```

* `x, y`: the vector(s) that contain the data (only one vector in the case of a one-sample t-test, two vectors otherwise).
* `alternative`: a character string specifying the alternative hypothesis, must be one of "two.sided" (default), "greater" or "less".
* `mu`: a number indicating the true value of the mean (or difference in means if you are performing a two sample test).
* `paired`: a logical indicating whether you want a paired t-test. 
* `var.equal`:a logical variable indicating whether to treat the two variances as being equal. If `TRUE` then the pooled variance is used to estimate the variance otherwise the Welch (or Satterthwaite) approximation to the degrees of freedom is used.
* `conf.level`: confidence level of the interval (default: 0.95).

ðŸ’¡ "paired" means there is an obvious and meaningful one-to-one correspondence between the data in the first set and the data in the second set, e.g. blood pressure in a set of patients before and after administering a medicine.

<u>6. Example</u>

Let us test if the mean BMI (`BMXBMI`) is significantly different between men and women (` RIAGENDR`).

```{r}
# We quickly check the distribution of the BMI using a Q-Q-plot (even though,
# with such a large sample, this isn't required)
qqnorm(y = merged_nhanes$BMXBMI)

# We create two vectors, with the BMIs of women and men
bmi_women <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Female"]
bmi_men <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Male"]

# We can start looking at how many non NA entries we have and by computing the means
sum(!is.na(bmi_women));sum(!is.na(bmi_men))
mean(bmi_women, na.rm = TRUE);mean(bmi_men, na.rm = TRUE)
qqplot(x = bmi_women, y = bmi_men)

# We compare their means using a t-test
t.test(x = bmi_women, y = bmi_men, alternative = 'two.sided', var.equal = TRUE, paired = FALSE)
```

* The `t` value is the statistic of the t-test.
* `df` are the degrees of freedom (a characteristics of the underlying distribution of the statistic), depends on the test used (one- or two-sample, paired or unpaired), in this case it is the sum of the two samples minus 2 (3239+3212-2=6449).
* The `p-value` is the probability of getting such a `t` value under the null hypothesis. It is very small: 5.854e-09, so to an $\alpha$-level of 0.05, we say that their is a statistically significant difference between the means of the two groups.
* The 95% CI tells us where the difference in the means lies, to a confidence of 95%. In coherence with the very low p-value, this interval does not contain 0 (which would mean "no difference").
* Finally, the means of both vectors are also computed.

ðŸ’¡ `qqnorm` is a function that produces a normal Q-Q-plot of the values in y. `qqplot` produces a Q-Q-plot of two datasets.

#### Z-test

The z-test is similar to the t-test (continuous variable(s), independent and normally distributed), and its null hypothesis is also that the means of two populations are equal, but it requires to know the standard deviation of your population(s). 

You should favour a z-test to a t-test if the **standard deviation** of the population is **known** and the sample size is greater than or equal to 30.

There is no z-test function available in base R, but in packages like `BSDA` (`z.test(x, ...)`).

### Wilcoxon rank-sum and signed-rank tests

Non-parametric (= makes minimal assumptions about the underlying distribution of the data being studied) "alternative" to the t-test (we do not test exactly the same thing, the t-test compares means, the Wilcoxon test compares distributions).

ðŸ’¡ If working on *paired data*, the test is called **Wilcoxon signed-rank test**, and on *unpaired data* it is called **Wilcoxon rank-sum test** (also called Mannâ€“Whitney U test, Mannâ€“Whitneyâ€“Wilcoxon test, or Wilcoxonâ€“Mannâ€“Whitney test).

<u>1. Null hypothesis</u>

* Distributions of both populations are identical

<u>2. Type of data</u>

* One or two vectors of continuous or ordinal variables (e.g. pain levels in two cohorts)

<u>3. Requirements</u>

* The data used to carry out the test should either be *sampled independently* from the two populations being compared or be fully paired. This is in general not testable from the data, but if the data are known to be dependent (e.g. paired by test design), a paired test has to be applied.
* The differences between paired observations should be *symmetrically distributed* around the median. Unlike the paired t-test, the Wilcoxon test does not require normality but works best when the distribution is roughly symmetric. 

<u>4. R function</u>

`wilcox.test(x, ...)`: Performs one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as â€˜Mann-Whitneyâ€™ test.

<u>5. Important arguments</u>

```
wilcox.test(x, y = NULL,
            alternative = c("two.sided", "less", "greater"),
            mu = 0, paired = FALSE, exact = NULL, correct = TRUE,
            conf.int = FALSE, conf.level = 0.95,
            tol.root = 1e-4, digits.rank = Inf, ...)
```

* `x, y`: the vector(s) that contain the data (only one vector in the case of a one-sample Wilcoxon test, two vectors otherwise).
* `alternative`: a character string specifying the alternative hypothesis, must be one of "two.sided" (default), "greater" or "less".
* `mu`: a number specifying an optional parameter used to form the null hypothesis. Set `mu = 0` (default) to test if the samples have the same distribution.
* `paired`: a logical indicating whether you want a paired test. If only x is given, or if both x and y are given and paired is TRUE, a **Wilcoxon signed rank test** of the null that the distribution of x (in the one sample case) or of x - y (in the paired two sample case) is symmetric about mu is performed. Otherwise, if both x and y are given and paired is FALSE, a **Wilcoxon rank sum test** (equivalent to the Mann-Whitney test: see the Note) is carried out. In this case, the null hypothesis is that the distributions of x and y differ by a location shift of mu and the alternative is that they differ by some other location shift.
* `conf.level`: confidence level of the interval (default: 0.95).

<u>6. Example</u>

Let us test if the distribution of the BMI (`BMXBMI`) is significantly different between men and women (` RIAGENDR`).

```{r}
# We create two vectors, with the BMIs of women and men
bmi_women <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Female"]
bmi_men <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Male"]

# We compare their distributions using a Wilcoxon test
wilcox.test(x = bmi_women, y = bmi_men, alternative = 'two.sided', mu = 0, paired = FALSE)
```

* The `W` value is the statistic of the Wilcoxon test (also called `U` statistic).
* The `p-value` is the probability of getting such a `W` value under the null hypothesis. It is very small: 0.0004466, so to an $\alpha$-level of 0.05, we say that their is a statistically significant difference between the distributions of the two groups.

## Categorical variables

### Chi-Squared test and Fisher's exact test

Both are statistical hypothesis tests used in the analysis of contingency tables. The Chi-Squared test can be used when the sample sizes are large, Fisher's exact test is valid for all sample sizes (even though in practice it is usually employed when sample sizes are small).

<u>1. Null hypothesis</u>

* There are no differences between the classes in the population (rows and columns are independent)

<u>2. Type of data</u>

* Two categorical variables (e.g. cohort/control and progression of tumour yes/no)

<u>3. Requirements</u>

* Use Fisher's exact test when the *expected count* in one or more of the cells in your contingency table is below 5.

ðŸ’¡ The expected counts are what we expect the numbers in the table to look like if the categorical variables are independent. The excepted count in a cell is always given by the formula: $$\text{expected count} = \frac{\text{sum of row} \cdot \text{sum of column}}{\text{total of table}}$$

Here an example:

**Observed frequencies table**

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;"></th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Non-smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Female</td>
      <td style="border: 1px solid black; padding: 8px;">406</td>
      <td style="border: 1px solid black; padding: 8px;">473</td>
      <td style="border: 1px solid black; padding: 8px;">879</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Male</td>
      <td style="border: 1px solid black; padding: 8px;">659</td>
      <td style="border: 1px solid black; padding: 8px;">737</td>
      <td style="border: 1px solid black; padding: 8px;">1396</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Total</td>
      <td style="border: 1px solid black; padding: 8px;">1065</td>
      <td style="border: 1px solid black; padding: 8px;">1210</td>
      <td style="border: 1px solid black; padding: 8px;">2275</td>
    </tr>
  </tbody>
</table>

<br>

**Expected frequencies table**

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;"></th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Non-smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Female</td>
      <td style="border: 1px solid black; padding: 8px;">411.49</td>
      <td style="border: 1px solid black; padding: 8px;">467.51</td>
      <td style="border: 1px solid black; padding: 8px;">879</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Male</td>
      <td style="border: 1px solid black; padding: 8px;">653.51</td>
      <td style="border: 1px solid black; padding: 8px;">742.49</td>
      <td style="border: 1px solid black; padding: 8px;">1396</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Total</td>
      <td style="border: 1px solid black; padding: 8px;">1065</td>
      <td style="border: 1px solid black; padding: 8px;">1210</td>
      <td style="border: 1px solid black; padding: 8px;">2275</td>
    </tr>
  </tbody>
</table>

<br>

<u>4. R function</u>

`chisq.test(x, ...)`: performs chi-squared contingency table tests and goodness-of-fit tests.

`fisher.test(x, ...)`: performs Fisher's exact test for testing the null of independence of rows and columns in a contingency table with fixed marginals.

<u>5. Important arguments</u>

```
chisq.test(x, y = NULL, correct = TRUE,
           p = rep(1/length(x), length(x)), rescale.p = FALSE,
           simulate.p.value = FALSE, B = 2000)
```

* `x`: a numeric vector or matrix. `x` and `y` can also both be factors.
* `y`: a numeric vector; ignored if x is a matrix. If `x` is a factor, `y` should be a factor of the same length.


```
fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,
            hybridPars = c(expect = 5, percent = 80, Emin = 1),
            control = list(), or = 1, alternative = "two.sided",
            conf.int = TRUE, conf.level = 0.95,
            simulate.p.value = FALSE, B = 2000)
```

* `x`: either a two-dimensional contingency table in matrix form, or a factor object.
* `y`: a factor object; ignored if x is a matrix.
* `alternative`: a character string specifying the alternative hypothesis, must be one of "two.sided" (default), "greater" or "less". Only used in the $2\times2$ case.
* `conf.int`: logical indicating if a confidence interval for the odds ratio in a $2\times2$ table should be computed (and returned).
* `conf.level`: confidence level for the returned confidence interval. Only used in the $2\times2$ case and if `conf.int = TRUE`.

<u>6. Example</u>

Let us test if the smoking status (`SMQ040`) is significantly different between men and women (` RIAGENDR`).

```{r}
# First, let us categorize the participants in smoking and non smoking
table(merged_nhanes$SMQ040)
merged_nhanes$SMOKE <- case_when(
  merged_nhanes$SMQ040 == "Not at all" ~ "No",
  merged_nhanes$SMQ040 %in% c("Every day", "Some days") ~ "Yes",
  merged_nhanes$SMQ040 == "Refused" ~ NA
)

# Let us visualize the contingency table
(tab <- table(merged_nhanes$SMOKE, merged_nhanes$RIAGENDR))

# What are the expected cell counts in this case?
chisq <- chisq.test(tab)
round(chisq$expected,2)

# Since all cells are >5, we can test the null hypothesis of independence using
# a chi-squared test
chisq.test(tab)

# Let us also try to run a Fisher's exact test on the same table
fisher.test(tab)
```

* As it was to be expected (the table of observed and expected frequencies were very similar), the Chi-Square test is non significant (`p-value` = 0.6669).
* Same conclusion for Fisher's exact test. The `estimate` given is an estimation of the odds ratio (only computed in the $2\times2$ case), with a corresponding `95% percent confidence interval`. Note that the $95\%$ confidence interval for the odds ratio contains 1 and that the `p-value` is above 0.05, which is coherent.
* The `X-squared` is the statistic of the Chi-Squared test (assumed to have a chi-squared distribution with `df` degrees of freedom). 
* For Fisher's exact test, the statistic is assumed to have a hypergeometric distribution.

#### McNemar's test

McNemar's test is similar to the Chi-Squared test ($2\times 2$ contingency table), and its null hypothesis is also that the variables are independent, but it is used on **paired** categorical data. It is often used in practice to compare the sensitivities and specificities for the evaluation of two diagnostic tests on the same group of patients.

The corresponding function in base R is called `mcnemar.test(x,...)`.

# Tests for more than two groups

## Kruskal-Wallis Test

Generalization of Wilcoxon-Mann-Whitney for more than two groups.

## Friedman Test

Generalization of paired tests (e.g., Wilcoxon) for more than two related groups.

## Pearson's Chi-Square Test

Complement Fisherâ€™s Exact Test, emphasizing itâ€™s better suited for larger samples.

# Tests for distribution and normality

## Lilliefors/ Kolmogorov-Smirnov-Lilliefors Test

Test for deviations from normality, set the stage for determining when to use parametric vs. non-parametric tests.

# Tests for survival analysis

## Logrank/ Log-Rank Test

For analyzing time-to-event data. Mention Kaplan-Meier curves for context.

## Correlation and association tests

## Correlation test by Pearson

Basis for understanding relationships between two continuous, normally distributed variables.

## Correlation test by Spearman

Non-parametric alternative for monotonic relationships.

# Predictive modeling and regression

## Generalized Linear Models (GLMs)

Purpose: GLMs are an extension of linear models that allow for non-normal distributions of the response variable (e.g., binary, count, or categorical outcomes). They offer more flexibility than traditional linear regression by using different link functions and error distributions.

Key Features: 

Linear relationship: GLMs assume a linear relationship between the predictors and the transformed response variable.

Link function: Links the linear predictor to the mean of the distribution. Common link functions:

* Identity link for normal distribution (linear regression)
* Logit link for binomial distribution (logistic regression)
* Log link for Poisson distribution (Poisson regression)

Error distributions: GLMs can be applied with various error distributions:
* Normal for continuous data (linear regression)
* Binomial for binary data (logistic regression)
* Poisson for count data

Assumptions

* Independence: Observations must be independent.
* Distribution: The response variable follows an appropriate distribution (e.g., binomial for binary outcomes, Poisson for count data).

Common Applications

* Linear regression: Predicting a continuous outcome.
* Logistic regression: Predicting binary outcomes (e.g., yes/no, success/failure).
* Poisson regression: Modeling count data (e.g., number of events in a fixed time period).
* Cox regression: A form of survival analysis used to model time-to-event data, often with censored observations. It is based on the proportional hazards assumption and estimates the effect of predictor variables on the hazard (event occurrence rate).

### Linear regression

Purpose: Used to model the relationship between a continuous dependent variable and one or more independent variables.
Assumptions: Linearity, normality of residuals, homoscedasticity, independence.
Example Application: Predicting the price of a house based on square footage, number of rooms, etc.

### Logistic regression

Purpose: Used when the dependent variable is binary (e.g., yes/no, success/failure).
Assumptions: Linear relationship between the log-odds of the outcome and predictors.
Example Application: Predicting the likelihood of a disease based on age, gender, and other factors.

### Cox proportional hazards regression

Purpose: Used for survival analysis, particularly when studying the time to an event (e.g., time to death, relapse).
Assumptions: Proportional hazards assumption, meaning the effect of the predictor on the hazard rate is constant over time.
Example Application: Analyzing the impact of age, treatment type, and other covariates on patient survival times.

### Multivariable regression

Purpose: An extension of linear or logistic regression with more than one predictor variable.
Assumptions: Similar to linear and logistic regression, but more complex due to multiple predictors.
Example Application: Predicting a health outcome (e.g., cholesterol levels) based on multiple lifestyle factors (e.g., diet, exercise, genetics).

## Mixed Effects Models

Purpose: Mixed effects models allow for the inclusion of both fixed and random effects, providing flexibility for hierarchical or grouped data. They are especially useful when there is variation between groups or subjects.

Key Features

Fixed effects: These are the main predictors of interest (e.g., treatment, age, etc.), which are assumed to have the same effect across all groups.
Random effects: These account for variability across groups or clusters (e.g., random intercepts for subjects or random slopes for measurements over time).

Assumptions

* Random effects are independent and identically distributed.
* Fixed effects have a linear relationship with the response variable.

Common Applications

* Longitudinal data: When measurements are taken repeatedly on the same subjects over time (e.g., repeated measurements on patients).
* Clustered data: When observations are grouped into clusters (e.g., students within schools, patients within hospitals).

## Generalized Additive Models (GAMs)

Purpose: GAMs extend GLMs by allowing for non-linear relationships between predictors and the outcome. This is useful when the relationship between the independent and dependent variables is not linear.

Key Features

Non-linear terms: Uses smooth functions (e.g., splines) for predictors, allowing for flexibility in modeling.

Additive structure: The model assumes that the total effect is an additive combination of linear and smooth non-linear terms.

Link function: Like GLMs, GAMs can use different link functions depending on the distribution of the outcome variable.

Common Applications: Modelling complex relationships in patient data where the effect of treatment or time may not be linear.

## Generalized Additive Mixed Models (GAMMs)

Purpose: GAMMs combine the flexibility of GAMs with random effects, useful for hierarchical or clustered data.

Key Features: Like GAMs, but with the inclusion of random effects to account for variability between groups.

Applications: Ideal for longitudinal studies or hierarchical data where both non-linear relationships and random effects are present.

Assumptions

* Additivity: The total effect is a sum of the individual effects of predictors (this can be both linear and smooth).
* Normality or appropriate error distribution: Depending on the type of outcome (e.g., Poisson for count data, binomial for binary data).
* Random effects: If included, random effects account for variations between groups or subjects.

Example Application: Analysing patient data where outcomes are influenced by both individual patient characteristics and random hospital-specific effects (e.g., variability between hospitals).

\newpage

# References

---
nocite: '@*'
...