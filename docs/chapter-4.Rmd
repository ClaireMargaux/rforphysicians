---
title: "Chapter 4"
date: "`r Sys.Date()`"
contact: claire.descombes@insel.ch
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    theme: paper
    code_folding: show
    math_method: katex
subtitle: "R for physicians"
bibliography: C:/GitHub/rforphysicians/docs/Rforphysicians.bib
link-citations: yes
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {rmarkdown::render(input)})
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE,
  message = FALSE, 
  fig.height = 5,
  fig.width = 6,
  eval = TRUE
)
library(tidyverse)
```

| Author           |                                                                 |
|:-----------------|:----------------------------------------------------------------|
| Name             | Claire Descombes                                                |
| Affiliation      | UniversitÃ¤tsklinik fÃ¼r Neurochirurgie, Inselspital Bern         |
| Degree           | MSc Statistics and Data Science, University of Bern             |
| Contact          | [claire.descombes@insel.ch](mailto:claire.descombes@insel.ch)   |

The reference material for this course, as well as some useful literature to deepen your knowledge of R, can be found at the bottom of the page.

---

To explore Râ€™s statistical tools, weâ€™ll continue working with the NHANES datasets. Specifically, weâ€™ll use the merged data frame that combines the `demo`, `bpx`, `bmx` and `smq` data sets. If you still have it loaded from Chapters 2 und 3, you can use it directly. Otherwise, you can download it from the [`data_sets` folder](https://github.com/ClaireMargaux/rforphysicians/tree/main/data_sets) and import it into R.

To be able to use survival data, we will also need the NHANES data set complemented with mortality data. If you still have it loaded from Chapters 3, you can use it directly. Otherwise, you can download it from the [`data_sets` folder](https://github.com/ClaireMargaux/rforphysicians/tree/main/data_sets) and import it into R.

```{r, echo=TRUE}
# Load the merged_nhanes CSV file
merged_nhanes <- read.csv("C:/GitHub/rforphysicians/data_sets/merged_nhanes.csv")

# Load the merged_nhanes_with_mort CSV file
merged_nhanes_with_mort <- read.csv("C:/GitHub/rforphysicians/data_sets/merged_nhanes_with_mort.csv")
```

# Most commonly used statistical tests and models

This chapter will follow the structure outlined below. We will learn how to perform a selection of statistical tests and models in R, categorising them by their objective (e.g. comparing two means or analysing survival).

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Section</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Topic</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Subtopics</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.1 Tests for comparing two groups</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests comparing means or proportions between two groups</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Continuous variables: Studentâ€™s t-test, Z-test, Wilcoxon rank-sum and signed-rank tests <br> â€¢ Categorical variables: Chi-square test, Fisherâ€™s exact test, McNemar's test, Cochran's Q test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.2 Tests for more than two groups</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for comparing multiple groups</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Continuous variables: One-way ANOVA, Repeated-measures ANOVA, Kruskal-Wallis test, Friedman test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.3 Tests for distribution and normality</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for distribution of data</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Shapiro-Wilk test, Kolmogorovâ€“Smirnov test, Lilliefors test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.4 Tests for survival analysis</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for time-to-event data</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Log-Rank test</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.5 Correlation and association tests</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Tests for relationships between variables</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> â€¢ Correlation test by Pearson <br> â€¢ Correlation test by Spearman</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>4.6 Regression</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Predictive models and regression techniques</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Generalized Linear Models (GLMs): Linear regression, logistic regression <br>â€¢ Cox (proportional hazards) regression</td>
    </tr>
  </tbody>
</table>

## Which test for which data?

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics(path = "images/types_of_data.jpg")
```

Before we begin, here is an overview of the type of data required by each test/model. More tests and models are presented than will be discussed in the course, but I have included them for the sake of completeness. If you feel lost on that topic at any point in the chapter, just scroll up again to have a look at this table.

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;"></th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">1 sample</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">2 paired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">2 unpaired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">&gt;2 paired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">&gt;2 unpaired samples</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Continuous predictor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Binary</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Binomial test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ McNemar test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square test<br>â€¢ Fisher's exact test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> </td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> </td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Logistic regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Nominal</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square goodness of fit test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Cochran's Q test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Chi-square test<br>â€¢ Extensions of Fisher's test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> </td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"> </td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Multinomial regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Ordinal</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ One-sample Wilcoxon signed-rank test<br>â€¢ One-sample Sign test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Wilcoxon signed-rank test<br>â€¢ Sign test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Wilcoxon rank-sum test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Friedman test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Kruskalâ€“Wallis test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Ordinal regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Continuous</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ One-sample t-test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Paired t-test<br>â€¢ Wilcoxon signed-rank test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Two-sample t-test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Repeated-measures ANOVA</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ One-way ANOVA</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Linear regression</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"><strong>Time-to-event</strong></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ One-sample log-rank test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Log-Rank test</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;"></td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">â€¢ Cox regression<br>â€¢ Weibull regression</td>
    </tr>
  </tbody>
</table>

## What is a statistical test?

Before we start looking at many different <u>statistical hypothesis tests</u>, let us recall the basic structure that they all share, by exploring more in-depth the Student's t-test for two, independent continuous samples.

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Generally</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Student's t-test for two, independent samples</th>
      </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">A test always aims to put to test a claim we make about some parameter (a correlation, a difference in mean, etc.) in the distribution of the population of interest.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Working on two patients cohorts, the smokers and the non-smokers, one could ask her-/himself if they share the same mean on a continuous variable, e.g. the BMI</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The data itself is expected to fulfil some **assumptions**.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">As the BMI data is continuous, the cohorts are big (> 100 patients each, of sizes $n_1$ and $n_2$) and assumed to be independent, and there is no reason to assume they'd have a different variance in the BMI, an unpaired Student's t-test appears to be the right choice.</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The 'status quo'/no difference formulation of the question we're asking is called the **null hypothesis**.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The null hypothesis, in our example, is the hypothesis that our two cohorts (the smokers and the non-smokers) share the same mean BMI</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The hypothesis we are usually interested to prove, the one showing a difference between two quantities, is called the **alternative hypothesis**</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">The alternative hypothesis, in our example, is the hypothesis that our two cohorts (the smokers and the non-smokers) do not share the same mean BMI</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Assuming the null hypothesis is true, a specific parameter, called the **statistic**, is expected to fall into a given **distribution**</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the null hypothesis is true, then the t statistic: $$t = \frac{\tilde{X}_1-\tilde{X}_2}{s_p \cdot \sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}$$ (where $s_p$ is the pooled standard deviation) follows a Student's t-distribution with $n_1+n_2-2$ degrees of freedom.</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">For any object of which the distribution is (assumed to be) known, it is possible to compute its probability to fall within a certain range; for a given **significance level** (denoted by $\alpha$) and between 0 and 1, one can determine an interval in which the statistic has a probability of $1-\alpha$ to fall within</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">Letting $\alpha$ be 0.05, as it is common in medicine, we can determine that our t statistic has a probability of 95% to be in a given interval (that can be computed in R or using a table).</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the statistic, after being computed, falls in the $1-\alpha$ confidence interval ($p>\alpha$), then we **cannot reject the null hypothesis**, and have to look for further evidence to prove our claim.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the t statistic falls into the range we computed, then we cannot say that we saw a significant difference in the means.</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the statistic, after being computed, does *not* fall in the $1-\alpha$ confidence interval ($p \leq \alpha$), then we can reject the null hypothesis, and say that the claim we made on the data is **statistically significant** with a $1-\alpha$ confidence.</td>
      <td style="border: 1px solid black; padding: 8px; vertical-align: top;">If the t-statistic does not fall into the range we computed, then we can say that we observe a statistically significant difference in the means, with a confidence of 95%.</td>
    </tr>    
  </tbody>
</table>

A few important comments: 

  + The p-value is the **probability under the null hypothesis** of obtaining a (real-valued) test statistic **at least as extreme** as the one obtained.
  + Loosely speaking, rejection of the null hypothesis implies that there is **sufficient evidence against it**. 
  + It is not possible to prove that the null hypothesis is true: it could always be that with a bigger effect size and/or a larger sample size, we could statistically show the difference that we are trying to demonstrate. So, when a test comes out "negatively" (no statistical significance, $p>\alpha$), it does not mean that our claim is necessarly wrong.
  + On the other hand, even a significant test ($p\leq\alpha$) does not mean that we *irrefutably proved* our claim: in a theoretical world, where we could repeat our experiment 100 times in the same conditions and on the same population, even if our theory is wrong, we would always find a "positive" result ($p\leq\alpha$) in $\alpha\%$ of the cases. That is why consensus, in science, always relies on repeated experiments that (for the vast majority of them*) draw the same conclusion.
  + \* Why is that? Because again, even in a theoretical world, where we could repeat our experiment 100 times in the same conditions, even if our theory is right, we would always find a "negative" result in $\alpha\%$ of the cases. 

```{r, echo=FALSE}
# parameters
mu <- 0
sigma <- 1

# significance: tails that are 2.5% each (two-tailed alpha = 0.05)
alpha <- 0.05
lower_cut <- qnorm(alpha/2, mean = mu, sd = sigma)
upper_cut <- qnorm(1 - alpha/2, mean = mu, sd = sigma)

# density data
x <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)
df <- data.frame(x = x, y = dnorm(x, mean = mu, sd = sigma))

# split regions
df_mid   <- df[df$x >  lower_cut & df$x <  upper_cut, ]
df_left  <- df[df$x <= lower_cut, ]
df_right <- df[df$x >= upper_cut, ]

# plot: middle first, tails after (so they sit on top), line on top of everything
ggplot(df, aes(x = x, y = y)) +
  geom_area(data = df_mid,   fill = "lightgreen", alpha = 0.6) +   # middle area
  geom_area(data = df_left,  fill = "coral",  alpha = 0.8) +   # left tail (on top)
  geom_area(data = df_right, fill = "coral",  alpha = 0.8) +   # right tail (on top)
  geom_line(size = 1) +
  geom_vline(xintercept = c(lower_cut, upper_cut), linetype = "dashed") +
  annotate("text", x = lower_cut, y = max(df$y)*0.9, label = sprintf("%.3f", lower_cut), hjust = 1.1) +
  annotate("text", x = upper_cut, y = max(df$y)*0.9, label = sprintf("%.3f", upper_cut), hjust = -0.1) +
  annotate("text", 
           x = (lower_cut + upper_cut)/2, 
           y = max(df$y)*0.5, 
           label = "95%", 
           size = 6, 
           fontface = "bold", 
           color = "black") +
  annotate("text", 
           x = lower_cut-1, 
           y = max(df$y)*0.5, 
           label = "2.5%", 
           size = 6, 
           fontface = "bold", 
           color = "black") +
  annotate("text", 
           x = upper_cut+1, 
           y = max(df$y)*0.5, 
           label = "2.5%", 
           size = 6, 
           fontface = "bold", 
           color = "black") +
  labs(title = "Standard normal density",
       x = "x", y = "Density") +
  theme_minimal(base_size = 14)
```

# Tests for comparing two groups

## Continuous variables

### Student's t-test

Parametric test used to evaluate whether the mean of two continuous variables is statistically significant or not. It provides an exact test for the equality of the means of two i.i.d. normal populations with unknown, but equal, variances.

<u>1. Null hypothesis</u>

* Means of two groups are equal

<u>2. Type of data</u>

* One or two vectors of continuous variables (e.g. blood pressure in mmHg in two cohorts)

<u>3. Requirements</u>

* Both samples are *approximately normally distributed*. Under weak assumptions, this follows in large samples from the central limit theorem, even when the distribution of observations in each group is non-normal.
* If using Student's original definition of the t-test, the two populations being compared should have the *same variance* (assessable graphically using a Qâ€“Q plot). If the sample sizes in the two groups being compared are equal, Student's original t-test is highly robust to the presence of unequal variances. *Welch's t-test* is insensitive to equality of the variances regardless of whether the sample sizes are similar.
* The data used to carry out the test should either be *sampled independently* from the two populations being compared or be fully paired. This is in general not testable from the data, but if the data are known to be dependent (e.g. paired by test design), a paired test has to be applied.

ðŸ’¡ Independent samples are randomly selected so that their observations are not dependent on the values of other observations.

<u>4. R function</u>

`t.test(x, ...)`: Performs one and two sample t-tests on vectors of data.

<u>5. Important arguments</u>

```
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)
```

* `x, y`: the vector(s) that contain the data (only one vector in the case of a one-sample t-test, two vectors otherwise).
* `alternative`: a character string specifying the alternative hypothesis, must be one of "two.sided" (default), "greater" or "less".
* `mu`: a number indicating the true value of the mean (or difference in means if you are performing a two sample test).
* `paired`: a logical indicating whether you want a paired t-test. 
* `var.equal`: a logical variable indicating whether to treat the two variances as being equal. If `TRUE` then the pooled variance is used to estimate the variance otherwise the Welch (or Satterthwaite) approximation to the degrees of freedom is used.
* `conf.level`: confidence level of the interval (default: 0.95).

ðŸ’¡ "paired" means there is an obvious and meaningful one-to-one correspondence between the data in the first set and the data in the second set, e.g. blood pressure in a set of patients before and after administering a medicine.

<u>6. Example</u>

Let us test if the mean BMI (`BMXBMI`) is significantly different between men and women (` RIAGENDR`).

```{r, out.width="75%"}
# We quickly check the distribution of the BMI using a Q-Q-plot (even though,
# with such a large sample, this isn't required)
qqnorm(y = merged_nhanes$BMXBMI)
# We will learn more on Q-Q-plots under "Assessments of distribution and normality"

# We create two vectors, with the BMIs of women and men
bmi_women <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Female"]
bmi_men <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Male"]

# We can start looking at how many non NA entries we have and by computing the means
sum(!is.na(bmi_women));sum(!is.na(bmi_men))
mean(bmi_women, na.rm = TRUE);mean(bmi_men, na.rm = TRUE)
qqplot(x = bmi_women, y = bmi_men)

# We compare their means using a t-test
t.test(x = bmi_women, y = bmi_men, alternative = 'two.sided', var.equal = TRUE, paired = FALSE)
```

* The `t` value is the statistic of the t-test.
* `df` are the degrees of freedom (a characteristics of the underlying distribution of the statistic), depends on the test used (one- or two-sample, paired or unpaired), in this case it is the sum of the two samples minus 2 (3239+3212-2=6449).
* The `p-value` is the probability of getting such a `t` value under the null hypothesis. It is very small: 5.854e-09, so to an $\alpha$-level of 0.05, we say that their is a statistically significant difference between the means of the two groups.
* The 95% CI tells us where the difference in the means lies, to a confidence of 95%. In coherence with the very low p-value, this interval does not contain 0 (which would mean "no difference").
* Finally, the means of both vectors are also computed.

ðŸ’¡ `qqnorm` is a function that produces a normal Q-Q-plot of the values in y. `qqplot` produces a Q-Q-plot of two datasets.

#### Z-test

The z-test is similar to the t-test (continuous variable(s), independent and normally distributed), and its null hypothesis is also that the means of two groups are equal, but it requires to know the standard deviation of your group(s). 

You should favour a z-test to a t-test if the **standard deviation** of the group is **known** and the sample size is greater than or equal to 30.

There is no z-test function available in base R, but in packages like `BSDA` (`z.test(x, ...)`).

### Wilcoxon rank-sum and signed-rank tests

Non-parametric (= makes minimal assumptions about the underlying distribution of the data being studied) "alternative" to the t-test (we do not test exactly the same thing, the t-test compares means, the Wilcoxon test compares distributions).

ðŸ’¡ If working on *paired data*, the test is called **Wilcoxon signed-rank test**, and on *unpaired data* it is called **Wilcoxon rank-sum test** (also called Mannâ€“Whitney U test, Mannâ€“Whitneyâ€“Wilcoxon test, or Wilcoxonâ€“Mannâ€“Whitney test).

<u>1. Null hypothesis</u>

* Distributions of both groups are identical (there is not one sample that stochastically dominates the other sample)

<u>2. Type of data</u>

* One or two vectors of continuous or ordinal variables (e.g. pain levels in two cohorts)

<u>3. Requirements</u>

* The data used to carry out the test should either be *sampled independently* from the two populations being compared or be fully paired. This is in general not testable from the data, but if the data are known to be dependent (e.g. paired by test design), a paired test has to be applied.
* The differences between paired observations should be *symmetrically distributed* around the median. Unlike the paired t-test, the Wilcoxon test does not require normality but works best when the distribution is roughly symmetric. 

<u>4. R function</u>

`wilcox.test(x, ...)`: Performs one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as â€˜Mann-Whitneyâ€™ test.

<u>5. Important arguments</u>

```
wilcox.test(x, y = NULL,
            alternative = c("two.sided", "less", "greater"),
            mu = 0, paired = FALSE, exact = NULL, correct = TRUE,
            conf.int = FALSE, conf.level = 0.95,
            tol.root = 1e-4, digits.rank = Inf, ...)
```

* `x, y`: the vector(s) that contain the data (only one vector in the case of a one-sample Wilcoxon test, two vectors otherwise).
* `alternative`: a character string specifying the alternative hypothesis, must be one of "two.sided" (default), "greater" or "less".
* `mu`: a number specifying an optional parameter used to form the null hypothesis. Set `mu = 0` (default) to test if the samples have the same distribution.
* `paired`: a logical indicating whether you want a paired test. If only x is given, or if both x and y are given and paired is TRUE, a **Wilcoxon signed rank test** of the null that the distribution of x (in the one sample case) or of x - y (in the paired two sample case) is symmetric about mu is performed. Otherwise, if both x and y are given and paired is FALSE, a **Wilcoxon rank sum test** (equivalent to the Mann-Whitney test: see the Note) is carried out. In this case, the null hypothesis is that the distributions of x and y differ by a location shift of mu and the alternative is that they differ by some other location shift.
* `conf.level`: confidence level of the interval (default: 0.95).

<u>6. Example</u>

Let us test if the distribution of the BMI (`BMXBMI`) is significantly different between men and women (` RIAGENDR`).

```{r}
# We create two vectors, with the BMIs of women and men
bmi_women <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Female"]
bmi_men <- merged_nhanes$BMXBMI[merged_nhanes$RIAGENDR == "Male"]

# We compare their distributions using a Wilcoxon test
wilcox.test(x = bmi_women, y = bmi_men, alternative = 'two.sided', mu = 0, paired = FALSE)
```

* The `W` value is the statistic of the Wilcoxon test (also called `U` statistic).
* The `p-value` is the probability of getting such a `W` value under the null hypothesis. It is very small: 0.0004466, so to an $\alpha$-level of 0.05, we say that their is a statistically significant difference between the distributions of the two groups.

## Categorical variables

### Chi-square test and Fisher's exact test

Both are non-parametric tests used in the analysis of contingency tables. They evaluate the difference between observed categorical data and expected categorical data (assumed randomly distributed) to determine if there is a significant difference or relationship between variables.

The Chi-square test can be used when the sample sizes are large, Fisher's exact test is valid for all sample sizes (even though in practice it is usually employed when sample sizes are small).

One can also test whether the observed proportions for a categorical variable differ from hypothesized proportions: this is called a Chi-square goodness of fit test.

<u>1. Null hypothesis</u>

* There are no differences between the classes in the population (rows and columns are independent)

<u>2. Type of data</u>

* Two categorical variables (e.g. cohort/control and progression of tumour yes/no)

<u>3. Requirements</u>

* Use Fisher's exact test when the *expected count* in one or more of the cells in your contingency table is below 5.

ðŸ’¡ The expected counts are what we expect the numbers in the table to look like if the categorical variables are independent. The excepted count in a cell is always given by the formula: $$\text{expected count} = \frac{\text{sum of row} \cdot \text{sum of column}}{\text{total of table}}$$

Here an example:

**Observed frequencies table**

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;"></th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Non-smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Female</td>
      <td style="border: 1px solid black; padding: 8px;">406</td>
      <td style="border: 1px solid black; padding: 8px;">473</td>
      <td style="border: 1px solid black; padding: 8px;">879</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Male</td>
      <td style="border: 1px solid black; padding: 8px;">659</td>
      <td style="border: 1px solid black; padding: 8px;">737</td>
      <td style="border: 1px solid black; padding: 8px;">1396</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Total</td>
      <td style="border: 1px solid black; padding: 8px;">1065</td>
      <td style="border: 1px solid black; padding: 8px;">1210</td>
      <td style="border: 1px solid black; padding: 8px;">2275</td>
    </tr>
  </tbody>
</table>

<br>

**Expected frequencies table**

<table style="width:100%; border-collapse: collapse; border: 1px solid black;">
  <thead>
    <tr>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;"></th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Non-smoking</th>
      <th style="border: 1px solid black; padding: 8px; text-align: left; vertical-align: top;">Total</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Female</td>
      <td style="border: 1px solid black; padding: 8px;">411.49</td>
      <td style="border: 1px solid black; padding: 8px;">467.51</td>
      <td style="border: 1px solid black; padding: 8px;">879</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Male</td>
      <td style="border: 1px solid black; padding: 8px;">653.51</td>
      <td style="border: 1px solid black; padding: 8px;">742.49</td>
      <td style="border: 1px solid black; padding: 8px;">1396</td>
    </tr>
    <tr>
      <td style="border: 1px solid black; padding: 8px;">Total</td>
      <td style="border: 1px solid black; padding: 8px;">1065</td>
      <td style="border: 1px solid black; padding: 8px;">1210</td>
      <td style="border: 1px solid black; padding: 8px;">2275</td>
    </tr>
  </tbody>
</table>

<br>

<u>4. R function</u>

`chisq.test(x, ...)`: performs chi-square contingency table tests and goodness-of-fit tests.

`fisher.test(x, ...)`: performs Fisher's exact test for testing the null of independence of rows and columns in a contingency table with fixed marginals.

<u>5. Important arguments</u>

```
chisq.test(x, y = NULL, correct = TRUE,
           p = rep(1/length(x), length(x)), rescale.p = FALSE,
           simulate.p.value = FALSE, B = 2000)
```

* `x`: a numeric vector or matrix. `x` and `y` can also both be factors.
* `y`: a numeric vector; ignored if x is a matrix. If `x` is a factor, `y` should be a factor of the same length.


```
fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,
            hybridPars = c(expect = 5, percent = 80, Emin = 1),
            control = list(), or = 1, alternative = "two.sided",
            conf.int = TRUE, conf.level = 0.95,
            simulate.p.value = FALSE, B = 2000)
```

* `x`: either a two-dimensional contingency table in matrix form, or a factor object.
* `y`: a factor object; ignored if x is a matrix.
* `alternative`: a character string specifying the alternative hypothesis, must be one of "two.sided" (default), "greater" or "less". Only used in the $2\times2$ case.
* `conf.int`: logical indicating if a confidence interval for the odds ratio in a $2\times2$ table should be computed (and returned).
* `conf.level`: confidence level for the returned confidence interval. Only used in the $2\times2$ case and if `conf.int = TRUE`.

<u>6. Example</u>

Let us test if the smoking status (`SMQ040`) is significantly different between men and women (` RIAGENDR`).

```{r}
# First, let us categorize the participants in smoking and non smoking
table(merged_nhanes$SMQ040)
merged_nhanes$SMOKE <- case_when(
  merged_nhanes$SMQ040 == "Not at all" ~ "No",
  merged_nhanes$SMQ040 %in% c("Every day", "Some days") ~ "Yes",
  merged_nhanes$SMQ040 == "Refused" ~ NA
)

# Let us visualize the contingency table
(tab <- table(merged_nhanes$SMOKE, merged_nhanes$RIAGENDR))

# What are the expected cell counts in this case?
chisq <- chisq.test(tab)
round(chisq$expected,2)

# Since all cells are >5, we can test the null hypothesis of independence using
# a chi-square test
chisq.test(tab)

# Let us also try to run a Fisher's exact test on the same table
fisher.test(tab)
```

* As it was to be expected (the table of observed and expected frequencies were very similar), the Chi-Square test is non significant (`p-value` = 0.6669).
* Same conclusion for Fisher's exact test. The `estimate` given is an estimation of the odds ratio (only computed in the $2\times2$ case), with a corresponding `95% percent confidence interval`. Note that the $95\%$ confidence interval for the odds ratio contains 1 and that the `p-value` is above 0.05, which is coherent.
* The `X-squared` is the statistic of the Chi-square test (assumed to have a chi-square distribution with `df` degrees of freedom). 
* For Fisher's exact test, the statistic is assumed to have a hypergeometric distribution.

#### McNemar's test, Cochran's Q test

McNemar's test is similar to the Chi-square test ($2\times 2$ contingency table), and its null hypothesis is also that the variables are independent, but it is used on **paired** categorical data. It is often used in practice to compare the sensitivities and specificities for the evaluation of two diagnostic tests on the same group of patients.

The corresponding function in base R is called `mcnemar.test(x,...)`.

The Cochran's Q test is an extension of the McNemar's test for more than two categories with paired responses ($n\times 2$ contingency table).

There is no function for a Cochran's Q test in base R, but some packages (`onewaytests`, `outliers`, etc.) can provide one.

# Tests for more than two groups

## Continuous variables

### One-way ANOVA (Analysis of Variance)

The one-way ANOVA is used as an extension to the t-test for differences among at least three groups. When there are only two means to compare, the t-test and the F-test (conducted by the one-way ANOVA) are equivalent.

<u>1. Null hypothesis</u>

* Means of different groups are equal

<u>2. Type of data</u>

* Vectors of continuous variables (e.g. blood pressure in mmHg in different cohorts)

<u>3. Requirements</u>

* The different samples are *approximately normally distributed*. Under weak assumptions, this follows in large samples from the central limit theorem, even when the distribution of observations in each group is non-normal.
* The groups being compared should have the *same variance*. If this is not the case, an approximate method of Welch (1951) can be used, which generalizes the commonly known 2-sample Welch test to the case of arbitrarily many samples.
* The data used to carry out the test should either be *sampled independently* from the populations being compared. ANOVA is not robust to violations to the assumption of independence. 

<u>4. R function</u>

`oneway.test(formula, data, ...)`: Test whether two or more samples from normal distributions have the same means.

<u>5. Important arguments</u>

```
oneway.test(formula, data, subset, na.action, var.equal = FALSE)
```

* `formula`: a formula of the form `lhs ~ rhs` where `lhs` gives the sample values and `rhs` the corresponding groups.
* `data`: an optional matrix or data frame containing the variables in the formula formula.
* `na.action`: a function which indicates what should happen when the data contain NAs.
* `var.equal`: a logical variable indicating whether to treat the variances in the samples as equal. If `TRUE`, then a simple F test for the equality of means in a one-way analysis of variance is performed. If `FALSE`, an approximate method of Welch (1951) is used, which generalizes the commonly known 2-sample Welch test to the case of arbitrarily many samples.

<u>6. Example</u>

Let us test if the mean blood pressure at first reading (`BPXSY1`) is significantly different between different age groups (` RIDAGEYR`).

```{r}
# We first define different age groups to compare them to each other
merged_nhanes <- merged_nhanes %>%
  mutate(AGE_GROUP = cut(RIDAGEYR, breaks = seq(10, 80, 10)))

# Let us have a look at the mean BP values (1st reading) for those groups
merged_nhanes %>%
  group_by(AGE_GROUP) %>%
  summarise(
    n = n(),
    mean_BPXSY1 = mean(BPXSY1, na.rm = TRUE)
  )

# Let us now test the means using a one-way ANOVA
oneway.test(BPXSY1 ~ AGE_GROUP, data = merged_nhanes)
```

* The `F` value is the statistic of the F-test conducted by the one-way ANOVA. It is assumed to have an F distribution.
* `num df` and `denom df` are the degrees of freedom (a characteristics of the underlying distribution of the statistic) for the numerator and the denominator composing the statistic (the F distribution has two different degrees of freedom). The degrees of freedom of the numerator	are the number of groups minus one, and the degrees of freedom of the denominator	are the total sample size minus the number of groups.
* The `p-value` is the probability of getting such an `F` value under the null hypothesis. It is very small: 2.2e-16, so to an $\alpha$-level of 0.05, we say that their is a statistically significant difference between the means of the different groups.


#### Repeated-measures ANOVA

Repeated measures ANOVA is the equivalent of the one-way ANOVA, but for related, not independent groups, and is the extension of the paired t-test.

It can be performed in R using the function `anova_test(data, formula, ...)`.


### Kruskal-Wallis test

Extension of the Wilcoxon rank-sum test for more than two groups.

<u>1. Null hypothesis</u>

* Distributions of the different groups are identical (there is not one sample that stochastically dominates one other sample)

<u>2. Type of data</u>

* Vectors of continuous or ordinal variables (e.g. pain levels in different cohorts)

<u>3. Requirements</u>

* The data used to carry out the test should be *sampled independently* from the populations being compared. If the data are known to be paired, a Friedman test has to be applied.
* The observations in each group should follow *similar distributions*.

<u>4. R function</u>

`kruskal.test(formula, data, subset, na.action, ...)`: Performs a Kruskal-Wallis rank sum test.

<u>5. Important arguments</u>

```
kruskal.test(formula, data, subset, na.action, ...)
```

* `formula`: a formula of the form `response ~ group` where response gives the data values and group a vector or factor of the corresponding groups.
* `data`: a matrix or data frame containing the variables in the formula formula.
* `na.action`: a function which indicates what should happen when the data contain NAs.

<u>6. Example</u>

Let us test if the mean blood pressure at first reading (`BPXSY1`) is significantly different between different age groups (` RIDAGEYR`), just like we did with the one-way ANOVA.

```{r, out.width="75%"}
# We first define different age groups to compare them to each other
merged_nhanes <- merged_nhanes %>%
  mutate(AGE_GROUP = cut(RIDAGEYR, breaks = seq(10, 80, 10)))

# Let us have a look at the distributions of the BP values (1st reading) for those groups
ggplot(merged_nhanes, aes(x = AGE_GROUP, y = BPXSY1)) +
  geom_boxplot(fill = "steelblue", alpha = 0.7) +
  labs(
    x = "Age group",
    y = "Systolic BP in mmHg",
    title = "Blood pressure (1st reading) by age group"
  ) +
  theme_minimal()

# Let us now test the distributions using a Kruskal-Wallis test
kruskal.test(BPXSY1 ~ AGE_GROUP, data = merged_nhanes)
```

* The `Kruskal-Wallis chi-squared` value is the statistic of the Kruskal-Wallis test (it is also called `H` statistic). In is assumed to follow a Chi-square distribution with $k-1$ (k beeing the number of groups) degrees of freedom (`df`).
* The `p-value` is the probability of getting such a value under the null hypothesis. It is very small: 2.2e-16, so to an $\alpha$-level of 0.05, we say that their is a statistically significant difference between the distributions of the different groups.


#### Friedman test

Generalization of paired tests (e.g., Wilcoxon) for more than two related groups. It is the non-parametric equivalent of the repeated-measures ANOVA.

It can be performed in R using the function `friedman.test(formula, data, ...)`.

## Categorical variables

There is no simple, direct statistical test to assess the independence of three or more categorical variables simultaneously. You can test pairwise independence between each pair of variables, but that doesnâ€™t guarantee joint independence.


# Assessments of distribution and normality

We will here discuss assessments for deviations from normality and determining when to use parametric versus non-parametric tests.

However, an important point to note is that: **If the sample size is large enough** ($>30$ or $40$), the violation of the normality assumption should not cause major problems. This implies that parametric procedures can be used even when the data are not normally distributed. The central limit theorem tells us that, in large samples, the sampling distribution (not the data distribution) tends to be normal regardless of the shape of the data, and the means of random samples from any distribution will themselves have a normal distribution.

We can visually check for normality using 

1. plots (comparing the data distribution to a normal distribution) or 
2. significance tests (hypothesis tests comparing the sample distribution to a normal one). 

It is important to ascertain whether the data show **serious deviation** from normality.

A general note on significance tests (reflecting my opinion, which has gained popularity over time): normality tests are **generally bad and should probably never be used**. This is because data in real life is never perfectly normal. Therefore, if you have a large enough sample size, you can always reject the null hypothesis that your data is normal. Conversely, if your sample size is too small, you will fail to reject the null hypothesis, even when its distribution is clearly not normal. In a sense, normality tests are therefore more a kind of sample size test.


## Visual assessments

Although visual inspection of the distribution can be used to assess normality, this approach does not guarantee that the distribution is normal. However, visual inspection is valuable for getting an initial impression of a data sample and can allow you to judge whether the data show a serious deviation from normality. 

There are many methods that can be used to visually assess (ab)normality:

* frequency distribution plots: histograms and box plots
* probability plots: P-P plots (probability-probability plots), Q-Q plots (quantile-quantile plots) 

We discussed histograms and box plots in Chapter 3; however, these are not the most suitable tools for assessing a given distribution.

A P-P plot compares the empirical cumulative distribution function of a dataset with a specified theoretical cumulative distribution function (e.g. the CDF of the normal distribution). A Q-Q plot compares the quantiles of a data distribution with the quantiles of a standardised theoretical distribution from a specified family of distributions (e.g. the normal distribution). 

As the P-P plot is only useful for comparing probability distributions with nearby or equal location parameters, our focus will be on Q-Q plots.

### Q-Q-plots

A Qâ€“Q plot (quantileâ€“quantile plot) is a probability plot, a graphical method for comparing two probability distributions by plotting their quantiles against each other. If the two distributions being compared are similar, the points in the Qâ€“Q plot will approximately lie on the **identity line** $y = x$. If the distributions are from the same family but differ in location or scale parameters (e.g. mean or standard deviation), the points will lie on a **line** $y = b + a \cdot x$, but not necessarily the identity line.

Qâ€“Q plots can be used to compare samples with each other or with theoretical distributions, particularly the normal distribution. Here, we will explore the use of Qâ€“Q plots for comparing data with a theoretical distribution, particularly the normal distribution. However, this method can also be used to compare two samples (e.g. to assess whether they have similar distributions, as mentioned when discussing the t-test).

ðŸ’¡ Quantiles are cut points that divide the range of a probability distribution into continuous intervals with equal probabilities. They can also be used to divide the observations in a sample in the same way. Common quantiles have special names: quartiles (four groups, with the median in the middle); deciles (ten groups); and percentiles (one hundred groups).

<u>1. Null hypothesis</u>

No null hypothesis here, since this is not a statistical test.

<u>2. Type of data</u>

Usually used on continuous numeric data, sometimes on discrete numeric data (with caution).

<u>3. Requirements</u>

No specific requirements.

<u>4. R function</u>

```
qqnorm(y, ylim, main = "Normal Q-Q Plot",
       xlab = "Theoretical Quantiles", ylab = "Sample Quantiles",
       plot.it = TRUE, datax = FALSE, ...)
       
qqplot(x, y, plot.it = TRUE,
       xlab = deparse1(substitute(x)),
       ylab = deparse1(substitute(y)), ...,
       conf.level = NULL, 
       conf.args = list(exact = NULL, simulate.p.value = FALSE,
                        B = 2000, col = NA, border = NULL))
```

`qqnorm` is a generic function that produces a normal Qâ€“Q plot of the values in `y`. It compares the sample to a *standard* normal distribution. This means that if your sample is approximately standard normally distributed, the points in the Qâ€“Q plot should lie on the identity line $y = x$. If your sample is approximately normally distributed (but not necessarily standard normal (i.e., with mean $\neq 0$ or standard deviation $\neq 1$), the points should lie on a line $y = b + a \cdot x$.

<u>5. Important arguments</u>

* `x`: The first sample for `qqplot` (absent in `qqnorm`, since we systematically compare our sample to the theoretical standard normal distribution).
* `y`: The second (`qqplot`) or only data (`qqnorm`) sample.

<u>6. Example</u>

```{r, out.width="75%"}
# Let us have a look at the distribution of weight and height with a Q-Q-plot.
qqnorm(y = merged_nhanes$BMXWT)
qqnorm(y = merged_nhanes$BMXHT)

# We can see that the height is more normally distributed than the weight,
# even though neither of them show a serious deviation from normality (none
# of them is particularly skewed).

# It would be equivalent to use the function qqplot and take a normally
# distributed vector as comparison.
simulated_normal <- rnorm(10000)
qqplot(x = simulated_normal, y = merged_nhanes$BMXWT)
qqplot(x = simulated_normal, y = merged_nhanes$BMXHT)
```


## Normality tests

The main tests for the assessment of normality are Kolmogorov-Smirnov (K-S) test, Lilliefors corrected K-S test and Shapiro-Wilk (S-W) test, but there are many more.

The K-S test compares the empirical cumulative distribution function (ECDF) of your sample with the CDF of a *fully specified (theoretical)* distribution, meaning that it cannot be used when you donâ€™t know the population mean or standard deviation. The Lilliefors test is a K-S test that allows you to estimate these parameters from your sample. 

A limitation of the K-S test is its high sensitivity to extreme values; the Lilliefors correction renders this test less conservative. Both tend to emphasize differences in the centre rather than tails.

The Shapiro-Wilk test is based on the correlation between the data and the corresponding normal scores and provides better power than the K-S test even after the Lilliefors correction. But the S-W test is very sensitive, especially to deviations in both skewness and kurtosis. It performs better for small to medium samples.

<u>1. Null hypothesis</u>

All those tests share the same null hypothesis: the data sample is (standard) normally distributed. A low p-value suggests that the sample does not come from a normal distribution.

<u>2. Type of data</u>

Continuous numeric data.

<u>3. Requirements</u>

The sample size plays a big role in the choice of the test and its interpretation, but there is no particular requirement on the data.

### Kolmogorovâ€“Smirnov test

The Kolmogorovâ€“Smirnov test evaluates whether a sample comes from a specific, fully specified continuous distribution, like the standard normal distribution. It cannot be used when you donâ€™t know the population mean or standard deviation.

<u>4. R function</u>

```
ks.test(x, y, ...,
        alternative = c("two.sided", "less", "greater"),
        exact = NULL, simulate.p.value = FALSE, B = 2000)
```

<u>5. Important arguments</u>

* `x`: a numeric vector of data values.
* `y`: either a numeric vector of data values, or a character string naming a cumulative distribution function or an actual cumulative distribution function such as `pnorm` (for comparison with the normal distribution; note that if `mean` or `sd` are not specified, we compare the sample with a *standard* normal distribution).

<u>6. Example</u>

```{r}
# We perform the Kolmogorov-Smirnov test on weight and height, comparing them
# to a standard normal distribution ("pnorm"), since we don't know their
# true mean and sd.
ks.test(merged_nhanes$BMXWT, "pnorm")
ks.test(merged_nhanes$BMXHT, "pnorm")

# As expected, those samples are not *standard* normal distributed.
# If we want to compare the sample to a normal distribution with mean and sd
# estimated from the sample, we need to use a Lilliefors test.
```

#### Lilliefors test

The Lilliefors test adapts the Kâ€“S test for normality when mean and SD are estimated from the sample. It corrects for the bias introduced when estimating parameters from the sample.

<u>4. R function</u>

There is no implementation of the K-S test in base R, we need the package `nortest` to access the function `lillie.test`.

```
lillie.test(x)
```

<u>5. Important arguments</u>

* `x`: a numeric vector of data values, the number of which must be greater than 4. Missing values are allowed.

<u>6. Example</u>

```{r}
library(nortest)

# We perform the Lilliefors (Kolmogorov-Smirnov) test on weight and height, 
# comparing them to a normal distribution.
lillie.test(merged_nhanes$BMXWT)
lillie.test(merged_nhanes$BMXHT)
```

This brings us to the topic of the usefulness of normality tests. Even though our height variable had a perfect Q-Q plot, the Lilliefors test is extremely sensitive here and, because of the large sample size, produces an extremely significant p-value.


### Shapiro-Wilk test

A test that assesses whether a sample of data comes from a normally distributed population. Like the Lilliefors test, it does not require knowledge of the true population mean and standard deviation, as these are estimated from the sample.

<u>4. R function</u>

```
shapiro.test(x)
```

Performs the Shapiro-Wilk test of normality.

<u>5. Important arguments</u>

* `x`: a numeric vector of data values. Missing values are allowed, but the number of non-missing values must be **between 3 and 5000**.

âš ï¸ The Shapiroâ€“Wilk test in R requires 3 to 5000 non-missing values because at least 3 points are needed to compute the statistic, and the implementationâ€™s coefficients and numerical accuracy are only valid up to 5000 observations.

<u>6. Example</u>

```{r}
# Since we have more than 5000 data points, we cannot formally use the S-W
# test on our weight or height; but in order to see how the output would look
# like (/!\ no statistical interpretation possible after that!), let us randomly
# select 5000 values from those variables.
BMXWT_5000 <-  sample(x = merged_nhanes$BMXWT, size = 5000, replace = FALSE)
BMXHT_5000 <-  sample(x = merged_nhanes$BMXHT, size = 5000, replace = FALSE)

# We perform the (statistically invalid) Shapiro-Wilk test on the randomly
# generated n=5000 weight and height.
shapiro.test(BMXWT_5000)
shapiro.test(BMXHT_5000)
```


# Tests for survival analysis

Before we delve deeper into the log-rank test, let's briefly discuss censoring, which is a crucial topic in time-to-event analysis.

### Time-to-event data, censoring

Survival analysis involves measuring the elapsed amount of time that passes between some defined â€œstart timeâ€ and some other defined â€œend pointâ€ for each observation (subject, individual, etc.) in the sample population. Quite often, there are observations in a survival analysis for which this elapsed amount of time is unknown. These observations are said to be censored, and the reasons this could occur are explained along with an example below to make it easier to understand.

Consider a hypothetical clinical trial that is scheduled to last for 12 months, and that will include 6 patients. Each patient was followed until one of three things happened:

1. The patient experienced the event of interest
2. The patient dropped out of the study
3. The study ended after 12 months, but the patient did not experience the event in that time

The graph below shows a timeline for each of the 6 patients in the study, along with an indication of whether or not they died (event) or were censored.

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics(path = "images/censoring.jpg")
```

Of this fictitious group of six patients, patients 1, 4 and 5 experienced the event during the study period. Patients 2 and 6 did not experience the event during the study period, but it is possible that they did so at a later date; however, this is not documented. Therefore, they are censored. Finally, patient 3 dropped out of the study, which is why he or she is also censored.

The time-to-event dataset would look like this:

```{r}
(survival_data <- data.frame(
  id = 1:6,
  time = c(7, 7, 8, 3, 6, 6),
  event = c(1, 1, 0, 1, 1, 0) # (1 = event, 0 = censored)
))
```

```{r, out.width="75%"}
library(survival) # for survfit
library(survminer) # for ggsurvplot

# Let us draw the KM curve and look at the number at risk.
fit <- survfit(Surv(time, event) ~ 1, data = survival_data)
ggsurvplot(fit,
           data = survival_data,
           palette = c("steelblue"),
           conf.int = TRUE,  
           xlab = "Time (months)",
           ylab = "Survival probability",
           risk.table = TRUE,
           title = "Kaplanâ€“Meier survival curve")
```

Censored patients are marked on the KM curves with small vertical ticks and do not affect the survival probability; only events do.

The number at risk is shown for each month, from 3 months (when the first patient experienced an event) to 8 months (the longest observation period in the cohort). At each time point, the number at risk is the number of patients who have not yet experienced the event.

## Log-Rank test

The log-rank test is a test for analysing time-to-event data. It compares the estimated hazard functions of two groups at each observed event time.

ðŸ’¡ The hazard function $h(t)$, at time $t$, is the instantaneous risk of experiencing the event among those who are still at risk. It is sometimes referred to as the hazard rate. We will discuss the hazard function in more detail when we look at Cox regression.

There are other ways to analyse time-to-event data (e.g. overall survival, recurrence-free survival, time to recurrence, etc.). Among the options, we have already mentioned are Kaplan-Meier curves (see Chapter 3), and we will look at Cox regression at the end of this chapter.

<u>1. Null hypothesis</u>

The null hypothesis is that the two groups have identical hazard functions.

<u>2. Type of data</u>

Time-to-event data for two groups, e.g. overall survival (OS) for "Treatment 1" and "Treatment 2".

<u>3. Requirements</u>

*  The event times of individuals in each group should be **independent** to each other. This assumption implies that the occurrence of an event (e.g., death or failure) for one individual should not influence the occurrence of an event for another individual.
* **Censoring** should **not be related** to the event being studied or to the group assignment (censored and non-censored patients do not differ in terms of their actual event times). The log-rank test assumes that the probability of censoring should be the same for all individuals within each group. 
* The hazard rates (the risk of an event occurring) for the compared groups should be consistent over time (we call that **proportional hazards** assumption). The ratio of the hazard rates should remain constant, indicating that the groups are not experiencing significantly different risks at different time points. 

<u>4. R function</u>

We need the package `survival` to perform a log-rank test.

```
survdiff(formula, data, subset, na.action, rho=0, timefix=TRUE)
```

<u>5. Important arguments</u>

* `formula`: a formula expression as for other survival models, of the form `Surv(time, status) ~ predictors`. 
* `data`: the data frame in which to interpret the variables occurring in the formula.
* `subset`: expression indicating which subset of the rows of data should be used in the fit. 

<u>6. Example</u>

```{r, out.width="75%"}
# We want to have a look at the survival of people depending on the variable
# SMQ020 (smoked at least 100 cigarettes in life).
# We keep only the responses 'yes' or 'no'.
df <- merged_nhanes_with_mort[merged_nhanes_with_mort$SMQ020 %in% c("Yes", "No"), ]

# We perform the log-rank test.
survdiff(Surv(PERMTH_INT, MORTSTAT) ~ SMQ020, data = df)

# Smoking has (unsurprisingly) an influence on death. To see in which direction
# (let us assume that we don't know), since the log-rank test does not tell
# us that, we draw the KM curve.
fit <- survfit(Surv(PERMTH_INT, MORTSTAT) ~ SMQ020, data = df)
ggsurvplot(fit,
           data = df,
           palette = c("steelblue", "salmon"),
           conf.int = TRUE,  
           xlab = "Time (months)",
           ylab = "Survival probability",
           ylim = c(0.7, 1),
           risk.table = TRUE,
           title = "Kaplanâ€“Meier survival curve")
```

# Correlation and association tests

Correlation coefficients are numerical measure of some type of **correlation**, meaning a statistical relationship (causal or not!) between two variables.

The null hypothesis of "no correlation" as measured by a correlation coefficient can be tested with a statistical hypothesis test.

## Correlation test by Pearson

The Pearson correlation coefficient $r$ is a correlation coefficient that measures **linear** correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance, such that the result always has a value between $âˆ’1$ and $1$:

* $-1$ indicates a perfectly negative linear correlation between two variables
* $0$ indicates no linear correlation between two variables
* $1$ indicates a perfectly positive linear correlation between two variables

âš ï¸ A Pearson's correlation coefficient of $0$ does **not** mean that there is no correlation between the two variables! Only that if there is one, it is not linear. Similarly, a non-significant p-value in the Pearson's correlation test ($p >> 0$) does not prove independence, only that there is no linear association between the variables.

ðŸ’¡ Covariance is a measure of the joint variability of two variables.

<u>1. Null hypothesis</u>

* The correlation coefficient $r$ is equal to $0$ (i.e. no linear correlation)

<u>2. Type of data</u>

* Two vectors of continuous variables (e.g. weight in [kg] and height in [m])

<u>3. Requirements</u>

* The Pearson's correlation *test* assumes that both variables are roughly *normally distributed*.
* Values in both data vectors must be *paired*, meaning that each element in one vector corresponds to a specific element in the other.

<u>4. R function</u>

`cor.test(x, ...)`: Test for association between paired samples, using one of Pearson's, Kendall's  or Spearman's correlation coefficient.

<u>5. Important arguments</u>

```
cor.test(x, y,
         alternative = c("two.sided", "less", "greater"),
         method = c("pearson", "kendall", "spearman"),
         exact = NULL, conf.level = 0.95, continuity = FALSE, ...)
```

* `x, y`: numeric vectors of data values. `x` and `y` must have the same length.
* `alternative`: indicates the alternative hypothesis and must be one of "two.sided", "greater" or "less". You can specify just the initial letter. "greater" corresponds to positive association, "less" to negative association.
* `method`: a character string indicating which correlation coefficient is to be used for the test. One of "pearson", "kendall", or "spearman", can be abbreviated.

<u>6. Example</u>

Let us measure the Pearson's correlation of different pairs of variables:

* `BMXWT`: Weight (kg)
* `BMXHT`: Standing height (cm)
* `BMXBMI`: Body Mass Index (kg/mÂ²)
* `BMXWAIST`: Waist circumference (cm)

```{r, out.width="75%"}
# We quickly check the distribution of weight, BMI, height and waist circumference using Q-Q-plots
qqnorm(y = merged_nhanes$BMXWT)
qqnorm(y = merged_nhanes$BMXHT)
qqnorm(y = merged_nhanes$BMXBMI)
qqnorm(y = merged_nhanes$BMXWAIST)

# Let us compute the Pearson's correlation coefficient and test.
# We also plot the each pair of variables in a scatter plot.
cor.test(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXHT, method = c("pearson")) # weight and height circumference
plot(merged_nhanes$BMXWT, merged_nhanes$BMXHT,
     col = "steelblue",
     xlab = "Weight (kg)",
     ylab = "Height (cm)",
     main = "Scatterplot of weight vs height")
cor.test(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXWAIST, method = c("pearson")) # weight and waist circumference
plot(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXWAIST,
     col = "steelblue",
     xlab = "Weight (kg)",
     ylab = "Waist circumference (cm)",
     main = "Scatterplot of weight vs waist circumference")
cor.test(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXWAIST, method = c("pearson")) # height and waist circumference
plot(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXWAIST,
     col = "steelblue",
     xlab = "Height (cm)",
     ylab = "Waist circumference (cm)",
     main = "Scatterplot of height vs waist circumference")
cor.test(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXBMI, method = c("pearson")) # weight and BMI
plot(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXBMI,
     col = "steelblue",
     xlab = "Weight (kg)",
     ylab = "Body Mass Index (kg/m^2)",
     main = "Scatterplot of weight vs BMI")
cor.test(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXBMI, method = c("pearson")) # height and BMI
plot(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXBMI,
     col = "steelblue",
     xlab = "Height (cm)",
     ylab = "Body Mass Index (kg/m^2)",
     main = "Scatterplot of height vs BMI")
```

* Weight and height ($r = 0.45$) show a moderate positive association, which makes sense, since taller individuals tend to weigh more.
* Weight and waist circumference ($r = 0.90$) show a very strong positive correlation, as expected, since heavier participants generally have larger waist measurements.
* Height and waist circumference ($r = 0.20$) only have a weak positive correlation: taller participants tend to have slightly larger waists, though the effect is small. Weight has much more effect on waist circumference.
* Weight and BMI ($r = 0.90$): very strong correlation, just like we expect, as BMI is directly derived from weight.
* Height and BMI ($r = 0.02, p = 0.09$): essentially no linear relationship, because BMI already accounts for height in its formula, which explains the near-zero correlation.

Note that while height and waist circumference appear roughly normally distributed, weight and BMI are slightly right-skewed, which may slightly affect the strength of Pearsonâ€™s correlation but not the overall interpretation of direction or relative magnitude.

## Correlation test by Spearman

The Spearman correlation coefficient $\rho$ between two variables is equal to the Pearson correlation coefficient $r$ between the **rank values** of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses **monotonic** (i.e. strictly increasing, but not necessarily linearly) relationships.

It also lies between $-1$ and $1$, and is interpreted similarly to the Pearson coefficient.

As it is a nonparametric test, ordinal data is also permitted.

ðŸ’¡ Ranking a vector of data means replacing its numerical or ordinal values by their rank, when the data are sorted. For example, the ranks of the numerical vector `c(3.4, 5.1, 2.6, 7.3)` are `c(2, 3, 1, 4)`. 

<u>1. Null hypothesis</u>

* The correlation coefficient $\rho$ is equal to $0$ (i.e. no monotonic correlation)

<u>2. Type of data</u>

* Two vectors of continuous or ordinal variables (e.g. weight in [kg] and height in [m])

<u>3. Requirements</u>

* Values in both data vectors must be *paired*, meaning that each element in one vector corresponds to a specific element in the other.

<u>4. R function</u>

`cor.test(x, ...)`: Test for association between paired samples, using one of Pearson's, Kendall's  or Spearman's correlation coefficient.

<u>5. Important arguments</u>

```
cor.test(x, y,
         alternative = c("two.sided", "less", "greater"),
         method = c("pearson", "kendall", "spearman"),
         exact = NULL, conf.level = 0.95, continuity = FALSE, ...)
```

* `x, y`: numeric vectors of data values. `x` and `y` must have the same length.
* `alternative`: indicates the alternative hypothesis and must be one of "two.sided", "greater" or "less". You can specify just the initial letter. "greater" corresponds to positive association, "less" to negative association.
* `method`: a character string indicating which correlation coefficient is to be used for the test. One of "pearson", "kendall", or "spearman", can be abbreviated.

<u>6. Example</u>

Let us measure the Spearman's correlation of different pairs of variables:

* `BMXWT`: Weight (kg)
* `BMXHT`: Standing height (cm)
* `BMXBMI`: Body Mass Index (kg/mÂ²)
* `BMXWAIST`: Waist circumference (cm)

```{r, out.width="75%"}
# Let us compute the Spearman's correlation coefficient and test.
# We also plot the each pair of variables in a scatter plot.
cor.test(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXHT, method = c("spearman"), exact = FALSE) # weight and height circumference
plot(merged_nhanes$BMXWT, merged_nhanes$BMXHT,
     col = "steelblue",
     xlab = "Weight (kg)",
     ylab = "Height (cm)",
     main = "Scatterplot of weight vs height")
cor.test(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXWAIST, method = c("spearman"), exact = FALSE) # weight and waist circumference
plot(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXWAIST,
     col = "steelblue",
     xlab = "Weight (kg)",
     ylab = "Waist circumference (cm)",
     main = "Scatterplot of weight vs waist circumference")
cor.test(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXWAIST, method = c("spearman"), exact = FALSE) # height and waist circumference
plot(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXWAIST,
     col = "steelblue",
     xlab = "Height (cm)",
     ylab = "Waist circumference (cm)",
     main = "Scatterplot of height vs waist circumference")
cor.test(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXBMI, method = c("spearman"), exact = FALSE) # weight and BMI
plot(x = merged_nhanes$BMXWT, y = merged_nhanes$BMXBMI,
     col = "steelblue",
     xlab = "Weight (kg)",
     ylab = "Body Mass Index (kg/m^2)",
     main = "Scatterplot of weight vs BMI")
cor.test(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXBMI, method = c("spearman"), exact = FALSE) # height and BMI
plot(x = merged_nhanes$BMXHT, y = merged_nhanes$BMXBMI,
     col = "steelblue",
     xlab = "Height (cm)",
     ylab = "Body Mass Index (kg/m^2)",
     main = "Scatterplot of height vs BMI")
```

* Weight and height ($\rho = 0.47$) have a slightly stronger correlation using Spearman's method than Pearson's ($r = 0.45$), suggesting a monotonic but not perfectly linear increase: heavier participants tend to be taller, though not in a strictly linear way.
* Weight and waist circumference ($\rho = 0.89$) have a Spearman's coefficient nearly identical to Pearson's ($r = 0.90$), confirming a very strong monotonic and rather linear association.
* Height and waist circumference ($\rho = 0.20$) have identical Spearman's and Pearsonâ€™s ($r = 0.20$) coefficients, suggesting a weak but positive and linear relationship.
* Weight and BMI ($\rho = 0.88$): their Spearman's coefficient is almost the same as their Pearson's coefficient ($r = 0.90$), again reflecting the strong link between weight and BMIâ€™s definition.
* Height and BMI ($\rho = 0.04, p < 0.001$): although statistically significant due to large sample size, the correlation remains basically nonexistent. There is practically no monotonic association.

The pattern of relationships is consistent between Pearsonâ€™s and Spearmanâ€™s coefficients. The slightly higher Spearman values (especially for weight vs. height) suggest some non-linearity or mild skewness in the data, which is consistent with the fact that weight and BMI are slightly right-skewed.

# Regression

You are probably already familiar with linear regression. However, linear regression can be viewed within a broader framework called Generalized Linear Models (GLMs). We will first lay the foundation for understanding GLMs before focusing on two GLMs that are widely used in medicine:

* Linear regression: for continuous outcomes (e.g. total lung capacity in liters).
* Logistic regression: for binary outcomes (e.g. presence or absence of a disease).

After discussing GLMs, we will move on to another type of regression model designed to handle time-to-event (i.e. censored) data: the Cox regression model (e.g. overall survival).

ðŸ’¡ In this lecture, we will refer to the variable that we aim to explain with our regression model as the **response variable** or **output variable**. The variables that we use to make this prediction will be called **input variables** or **explanatory variables**. Different fields (medicine, statistics, economics, etc.) may use other names for these variables, which we summarize in the list below.

âš ï¸ I strongly advise *against* using the terms *dependent and independent* variables, because these terms can misleadingly imply that some variables are fully independent of others or suggest a causal relationship.

| Field                       | Output Variable            | Input Variable                     |
| --------------------------- | -------------------------- | ---------------------------------- |
| Statistics / Data Science   | Response variable          | Predictor variable / Covariate     |
| Medicine / Epidemiology     | Outcome                    | Risk factor / Exposure / Covariate |
| Economics / Social Sciences | Dependent variable         | Independent variable / Regressor   |
| Machine Learning            | Target                     | Feature / Input                    |
| General / Other fields      | Output                     | Input / Explanatory                |

## Generalized Linear Models (GLMs)

Generalized linear models (GLMs) are an extension of linear models that 

1. allow for response variables with *arbitrary distributions* (rather than simply normal distributions)
2. and an arbitrary function of the response variable (the *link function*) that varies linearly with the predictors (rather than the response itself being assumed to vary linearly). 

In short: They offer more flexibility than traditional linear regression.

| Model type | Formula | Description |
|-------------|----------|-------------|
| **Linear Regression** | $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \varepsilon_i, \ i = 1,...,n$ | Models a continuous outcome directly as a linear combination of predictors. Errors $\varepsilon_i$ are assumed normally distributed. |
| **Generalized Linear Model (GLM)** | $g(\mathbb{E}[y_i]) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}, \ i = 1,...,n$ | Models the *expected value* ($\mathbb{E}$) of the outcome via a *link function* $g(\cdot)$, each outcome $y_i$ is assumed to be generated from a particular distribution (e.g., normal, binomial, Poisson). |

ðŸ’¡ Notes:

* $y_i$ is the response variable.
* $x_{ij}$ are the explanatory (input) variables.
* $g(\cdot)$ is the link function, e.g.
  * Identity link â†’ linear regression
  * Logit link â†’ logistic regression
  * Log link â†’ Poisson regression

GLMs consist of three elements:

1. Linear relationship: GLMs assume a linear relationship between the predictors and the transformed response variable.
2. Link function $g(\cdot)$: Links the linear predictor to the mean of the distribution. Common link functions:

* Identity link for normal distribution (linear regression): $g(p)=p$
* Logit link for binomial distribution (logistic regression): $g(p)=\text{logit}(p)=\ln\left(\frac{p}{1-p}\right)$
* Log link for Poisson distribution (Poisson regression): $g(p)=\text{log}(p)$

3. Output distribution: GLMs can be applied with various distributions for modelling the output:

* Normal for continuous data (linear regression)
* Binomial for binary data (logistic regression)
* Poisson for count data (Poisson regression)

<u>1. Null hypothesis</u>

Regression as a whole does not have a single null hypothesis, it is primarily a method for estimating coefficients that best fit the data.
However, each coefficient in the model can be tested individually under the null hypothesis that it equals zero (i.e. that the corresponding explanatory variable has no effect on the response variable).

<u>2. Type of data</u>

The type of regression used within the GLM framework depends on the nature of the response variable:

* Continuous â†’ Linear regression (e.g. blood pressure, cholesterol level)
* Binary â†’ Logistic regression (e.g. smoker vs. non-smoker)
* Count / Discrete â†’ Poisson regression (e.g. number of hospital visits)

<u>3. Requirements</u>

* Observations must be *independent*.
* The response variable follows an *appropriate distribution* (e.g., binomial for binary outcomes, Poisson for count data).

âš ï¸ A possible point of confusion has to do with the distinction between **generalized linear models** (discussed here) and **general (multivariate) linear models** (which is simply a compact way of simultaneously writing several multiple linear regression models). 

### Linear regression

From the perspective of generalized linear models, it is useful to suppose that the distribution function is the normal distribution with constant variance and the link function is the identity: under these assumptions, the least-squares estimator is obtained as the maximum-likelihood parameter estimate.

<u>2. Type of data</u>

Continuous response variable.

<u>3. Requirements</u>

* Linearity: the relationship between each predictor and outcome must be linear.
* Normality of residuals: the residuals should follow a normal distribution.
* Constant variance (a.k.a. homoscedasticity): the variance of the residuals does not depend on the values of the predictor variables.
* Independence: the observations should be independent (not repeated measures).

ðŸ’¡ Residuals are the difference between a real-world observation and a value predicted by a model.


### Logistic regression

<u>2. Type of data</u>

Binary response variable.

<u>3. Requirements</u>

* Linearity: the relationship between the log-odds of the outcome and each predictor must be linear.
* Independence: the observations should be independent (not repeated measures).
* XXX


## Cox (proportional hazards) regression

Purpose: Used for survival analysis, particularly when studying the time to an event (e.g., time to death, relapse).
Assumptions: Proportional hazards assumption, meaning the effect of the predictor on the hazard rate is constant over time.
Example Application: Analysing the impact of age, treatment type, and other covariates on patient survival times.

### Hazard function

### Competing risks


## Exercices

Comparing two groups â€“ t-test
Compare the mean systolic blood pressure (BPXSY1) between males and females.

Which test would you use if the data is roughly normal?

How about if it is skewed?

Comparing two groups â€“ Wilcoxon test
Compare waist circumference (BMXWAIST) between participants who smoke at least 100 cigarettes in life (SMQ020 == "Yes") and non-smokers.

Comparing proportions â€“ Chi-square test
Test whether smoking status (SMQ020) differs by gender (RIAGENDR).

Comparing proportions â€“ Fisherâ€™s exact test
Test the association between cigar smoking (SMQ680) and pipe smoking (SMQ690).

Comparing more than two groups â€“ ANOVA
Compare mean BMI (BMXBMI) across age groups (AGE_GROUP).

Comparing more than two groups â€“ Kruskal-Wallis test
Compare waist circumference (BMXWAIST) across different smoking frequency categories (SMQ040).

Test for normality
Check if BMI (BMXBMI) is normally distributed using the Shapiro-Wilk test.

Correlation test â€“ Pearson
Compute the Pearson correlation between weight (BMXWT) and BMI (BMXBMI).

Correlation test â€“ Spearman
Compute the Spearman correlation between height (BMXHT) and waist circumference (BMXWAIST).

Survival analysis â€“ Kaplanâ€“Meier
Using the NHANES mortality data (MORTSTAT, PERMTH_INT), plot the Kaplanâ€“Meier survival curve by gender.

Optional: Perform a Log-Rank test to compare survival curves.

Bonus â€“ Linear regression
Fit a linear regression model predicting systolic blood pressure (BPXSY1) using age (RIDAGEYR) and BMI (BMXBMI) as predictors.

Interpret the coefficients.

Bonus â€“ Logistic regression
Predict the probability of being a smoker (SMQ020 == "Yes") using age and BMI as predictors.


\newpage

# References

---
nocite: '@*'
...